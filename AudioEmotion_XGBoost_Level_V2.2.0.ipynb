{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2otJOzxuaQ/snK4sYd3ye"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"g7P-72XRFNgb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726060927606,"user_tz":-480,"elapsed":15122,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"2f5b9fe9-59c7-409a-ca8b-fd86cfde7b56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opensmile in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: audobject>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from opensmile) (0.7.11)\n","Requirement already satisfied: audinterface>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from opensmile) (1.2.2)\n","Requirement already satisfied: audeer>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (2.2.0)\n","Requirement already satisfied: audformat<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.0)\n","Requirement already satisfied: audiofile>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.5.0)\n","Requirement already satisfied: audmath>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.4.1)\n","Requirement already satisfied: audresample<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.3)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (8.4.0)\n","Requirement already satisfied: oyaml in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (24.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audeer>=1.18.0->audinterface>=0.7.0->opensmile) (4.66.5)\n","Requirement already satisfied: iso-639 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (0.4.5)\n","Requirement already satisfied: iso3166 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.1.1)\n","Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (14.0.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.26.4)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.20.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Git LFS initialized.\n","fatal: destination path 'CREMA-D' already exists and is not an empty directory.\n"]}],"source":["!pip install opensmile\n","!pip install --upgrade pandas\n","!pip install xgboost\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://github.com/CheyneyComputerScience/CREMA-D.git"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import opensmile\n","import audiofile\n","from xgboost import XGBClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"metadata":{"id":"W_WLX9NBFUOl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726060930498,"user_tz":-480,"elapsed":2895,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"d632f0b9-a101-4447-fda5-4e3c0c7a0e3d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"]}]},{"cell_type":"code","source":["# AudioProcessor: Handles loading of audio files\n","class AudioProcessor:\n","  \"\"\"\n","  AudioProcessor handles loading of audio files.\n","  It extracts audio signals and sampling rates from audio files.\n","  \"\"\"\n","\n","  def __init__(self, file_paths):\n","    self.file_paths = file_paths  # List of audio file paths\n","\n","  def load_audio(self, path):\n","    \"\"\"\n","    Loads an audio file and returns the signal and sampling rate.\n","    \"\"\"\n","    try:\n","      signal, sampling_rate = audiofile.read(path, always_2d=True)\n","    except Exception as e:\n","      print(f\"Error loading {path}: {str(e)}\")\n","      return None, None\n","    return signal, sampling_rate\n","\n","  def batch_load(self):\n","    \"\"\"\n","    Loads all audio files in batch.\n","    Returns a list of tuples containing the audio signals and sampling rates.\n","    \"\"\"\n","    signals = []\n","    for path in self.file_paths:\n","      signal, sampling_rate = self.load_audio(path)\n","      if signal is not None:\n","        signals.append((signal, sampling_rate))\n","    return signals\n","\n","\n","# FeatureExtractor: Uses OpenSmile to extract features from audio\n","class FeatureExtractor:\n","  \"\"\"\n","  Extracts features from audio files using OpenSmile.\n","  \"\"\"\n","\n","  def __init__(self):\n","    self.smile = opensmile.Smile(\n","        feature_set=opensmile.FeatureSet.eGeMAPSv02,\n","        feature_level=opensmile.FeatureLevel.Functionals\n","    )\n","\n","  def extract_features(self, signal, sampling_rate):\n","    \"\"\"\n","    Extracts features from a single audio signal using OpenSmile.\n","    \"\"\"\n","    features = self.smile.process_signal(signal, sampling_rate)\n","    return features\n","\n","  def extract_batch_from_paths(self, paths):\n","    \"\"\"\n","    Extracts features from a list of audio file paths.\n","    \"\"\"\n","    all_features = []\n","    for path in paths:\n","      signal, sampling_rate = audiofile.read(path, always_2d=True)\n","      if signal is not None:\n","        features = self.extract_features(signal, sampling_rate)\n","        all_features.append(features)\n","    return pd.concat(all_features, ignore_index=True)\n","\n","\n","# EmotionClassifier: XGBoost classifier with RandomizedSearchCV for hyperparameter tuning\n","class EmotionClassifier:\n","  \"\"\"\n","  A classifier for predicting emotions using XGBoost with RandomizedSearchCV for faster hyperparameter tuning.\n","  \"\"\"\n","\n","  def __init__(self):\n","    self.model = XGBClassifier(random_state=42)\n","    self.label_encoder = LabelEncoder()\n","    self.scaler = StandardScaler()\n","\n","  def train(self, X_train, y_train):\n","    \"\"\"\n","    Trains the emotion classifier using scaled features and encoded labels.\n","    Uses RandomizedSearchCV for more comprehensive hyperparameter tuning.\n","    \"\"\"\n","    y_train_encoded = self.label_encoder.fit_transform(y_train)\n","    X_train_scaled = self.scaler.fit_transform(X_train)\n","\n","    # Define a hyperparameter grid\n","    param_distributions = {\n","        'n_estimators': [100, 200],\n","        'max_depth': [3, 5],\n","        'learning_rate': [0.01, 0.05, 0.1]\n","    }\n","\n","    # Use RandomizedSearchCV with more iterations\n","    randomized_search = RandomizedSearchCV(\n","        estimator=self.model,\n","        param_distributions=param_distributions,\n","        n_iter=5,\n","        cv=3,\n","        scoring='accuracy',\n","        verbose=2,\n","        n_jobs=-1\n","    )\n","    randomized_search.fit(X_train_scaled, y_train_encoded)\n","\n","    # Use the best model from RandomizedSearchCV\n","    self.model = randomized_search.best_estimator_\n","    print(f\"Best parameters found: {randomized_search.best_params_}\")\n","\n","    # Evaluate cross-validation scores\n","    cv_scores = cross_val_score(self.model, X_train_scaled, y_train_encoded, cv=3, scoring='accuracy')\n","    print(f\"Cross-validation scores: {cv_scores}\")\n","    print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predicts emotions on new data and returns a list of PredictionResult objects.\n","    \"\"\"\n","    X_scaled = self.scaler.transform(X)\n","    y_pred_encoded = self.model.predict(X_scaled)\n","    y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n","    y_proba = self.model.predict_proba(X_scaled)\n","    confidence_levels = y_proba.max(axis=1)\n","    return [PredictionResult(label, confidence) for label, confidence in zip(y_pred, confidence_levels)]\n","\n","\n","# PredictionResult: Stores emotion classification results\n","class PredictionResult:\n","  \"\"\"\n","  Stores the result of an emotion prediction.\n","  \"\"\"\n","\n","  def __init__(self, label, confidence):\n","    self.label = label  # Predicted emotion label\n","    self.confidence = round(confidence, 2)  # Confidence score\n","\n","  def __repr__(self):\n","    \"\"\"\n","    String representation of the prediction result.\n","    \"\"\"\n","    return f\"PredictionResult(label={self.label}, confidence={self.confidence:.2f})\"\n","\n","\n","# AudioEmotionDetectionPipeline: Get results\n","class AudioEmotionDetectionPipeline:\n","  \"\"\"\n","  Manages the workflow:\n","  - Extracts features using OpenSmile.\n","  - Trains a model using CREMA-D AudioMP3 files.\n","  - Predicts emotions on new audio files using the trained model.\n","  \"\"\"\n","\n","  def __init__(self, file_ids):\n","    self.file_ids = file_ids  # Google Drive audio file IDs\n","    self.processor = None  # To handle audio file processing\n","    self.extractor = FeatureExtractor()  # To extract features from audio\n","    self.classifier = EmotionClassifier()  # Emotion classifier\n","\n","  def load_crema_d_data(self):\n","    \"\"\"\n","    Loads CREMA-D AudioMP3 dataset, extracting file paths, emotion labels, and emotion levels from filenames.\n","    Returns a DataFrame with file paths, combined emotion labels and levels.\n","    \"\"\"\n","    audio_dir = './CREMA-D/AudioMP3'\n","    audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n","\n","    # Define emotion and level mappings\n","    emotions = {\n","        'ANG': 'Anger',\n","        'DIS': 'Disgust',\n","        'FEA': 'Fear',\n","        'HAP': 'Happiness',\n","        'NEU': 'Neutral',\n","        'SAD': 'Sadness'\n","    }\n","\n","    levels = {\n","        'LO': 'Low',\n","        'MD': 'Medium',\n","        'HI': 'High',\n","        'XX': 'Unspecified'\n","    }\n","\n","    file_paths = []\n","    labels = []\n","\n","    for file in audio_files:\n","        parts = file.split('_')\n","\n","        if len(parts) >= 4:\n","            emotion_code = parts[2]  # The third part is the emotion\n","            level_code = parts[3].replace('.mp3', '')  # Remove the .mp3 extension\n","\n","            if emotion_code in emotions and level_code in levels:\n","                emotion = emotions[emotion_code]\n","                level = levels[level_code]\n","                combined_label = f\"{emotion}_{level}\"  # Combine emotion and level\n","\n","                file_paths.append(os.path.join(audio_dir, file))\n","                labels.append(combined_label)\n","\n","    print(f\"Loaded {len(labels)} labels from the files.\")\n","\n","    return pd.DataFrame({'Path': file_paths, 'Label': labels})\n","\n","  def download_and_extract_features(self):\n","    \"\"\"\n","    Downloads audio files from Google Drive and extracts features.\n","    Returns a DataFrame with extracted features.\n","    \"\"\"\n","    file_paths = self.download_files_from_drive(self.file_ids)\n","    self.processor = AudioProcessor(file_paths)\n","    return self.extractor.extract_batch_from_paths(file_paths)\n","\n","  def download_files_from_drive(self, file_ids):\n","    \"\"\"\n","    Downloads files from Google Drive using file IDs.\n","    Returns a list of file paths.\n","    \"\"\"\n","    auth.authenticate_user()\n","    gauth = GoogleAuth()\n","    gauth.credentials = GoogleCredentials.get_application_default()\n","    drive = GoogleDrive(gauth)\n","\n","    file_paths = []\n","    for filename, file_id in file_ids.items():\n","      downloaded = drive.CreateFile({'id': file_id})\n","      downloaded.GetContentFile(filename)\n","      file_paths.append(filename)\n","      print(f\"{filename} downloaded\")\n","    return file_paths\n","\n","  def train_classifier(self):\n","    \"\"\"\n","    Trains the emotion classifier using CREMA-D dataset.\n","    \"\"\"\n","    crema_d_data = self.load_crema_d_data()\n","\n","    # Debugging print: Check the size of the dataset before splitting\n","    print(f\"Dataset size before splitting: {crema_d_data.shape}\")\n","\n","    if crema_d_data.empty:\n","      print(\"Error: The dataset is empty!\")\n","      return\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        crema_d_data['Path'], crema_d_data['Label'], test_size=0.2, random_state=42)\n","\n","    # Extract features for training and testing\n","    X_train_features = self.extractor.extract_batch_from_paths(X_train)\n","    X_test_features = self.extractor.extract_batch_from_paths(X_test)\n","\n","    print(f\"Shape of training features: {X_train_features.shape}\")\n","    print(f\"Shape of testing features: {X_test_features.shape}\")\n","\n","    self.classifier.train(X_train_features, y_train)\n","\n","    # Evaluate model performance\n","    y_test_pred = self.classifier.predict(X_test_features)\n","    print(\"Model evaluation on test set:\")\n","    print(classification_report(y_test, [result.label for result in y_test_pred]))\n","\n","    cm = confusion_matrix(y_test, [result.label for result in y_test_pred])\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","\n","  def run(self):\n","    \"\"\"\n","    Runs the entire pipeline and returns predictions.\n","    \"\"\"\n","    # Train classifier and predict on new audio files\n","    self.train_classifier()\n","    audio_features = self.download_and_extract_features()\n","\n","    # Predict on new audio files\n","    predictions = self.classifier.predict(audio_features)\n","\n","    # Split the combined label into emotion and level\n","    results_df = pd.DataFrame([{\n","        \"emotion\": pred.label.split('_')[0],\n","        \"level\": pred.label.split('_')[1],\n","        \"confidence\": pred.confidence\n","    } for pred in predictions])\n","\n","    return results_df"],"metadata":{"id":"riEbjX1hFUdK","executionInfo":{"status":"ok","timestamp":1726060930498,"user_tz":-480,"elapsed":2,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Main function to run the pipeline\n","def main():\n","  \"\"\"\n","  Main function that runs the entire emotion recognition pipeline.\n","  \"\"\"\n","  # Define Google Drive file IDs (replace with actual file IDs)\n","  file_ids = {\n","      'audio1.mp3': '108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r',\n","      'audio2.mp3': '13O1hKhYl5Uzlb0mIadH5hv5t_zSud664'\n","  }\n","\n","  # Create and run the AudioEmotionDetectionPipeline\n","  pipeline = AudioEmotionDetectionPipeline(file_ids)\n","  results_df = pipeline.run()\n","\n","  # Output the results\n","  print(results_df)\n","\n","\n","if __name__ == \"__main__\":\n","  main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vcctfyg6B5qS","executionInfo":{"status":"ok","timestamp":1726063714272,"user_tz":-480,"elapsed":2783776,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"6112b1c4-9441-4d25-da5b-75b8e0030a46"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 7442 labels from the files.\n","Dataset size before splitting: (7442, 2)\n","Shape of training features: (5953, 88)\n","Shape of testing features: (1489, 88)\n","Fitting 3 folds for each of 5 candidates, totalling 15 fits\n","Best parameters found: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05}\n","Cross-validation scores: [0.4534005  0.43447581 0.43951613]\n","Mean cross-validation score: 0.44246414642073617\n","Model evaluation on test set:\n","                       precision    recall  f1-score   support\n","\n","           Anger_High       0.91      0.56      0.69        18\n","            Anger_Low       0.17      0.05      0.07        21\n","         Anger_Medium       0.12      0.05      0.07        22\n","    Anger_Unspecified       0.58      0.78      0.66       181\n","         Disgust_High       0.17      0.05      0.08        19\n","          Disgust_Low       0.00      0.00      0.00        13\n","       Disgust_Medium       0.11      0.05      0.07        20\n","  Disgust_Unspecified       0.42      0.41      0.42       183\n","            Fear_High       0.27      0.27      0.27        11\n","             Fear_Low       0.17      0.07      0.10        15\n","          Fear_Medium       0.20      0.05      0.07        22\n","     Fear_Unspecified       0.49      0.39      0.43       210\n","       Happiness_High       0.50      0.04      0.08        24\n","        Happiness_Low       0.14      0.06      0.08        17\n","     Happiness_Medium       0.33      0.05      0.09        19\n","Happiness_Unspecified       0.51      0.55      0.53       231\n","  Neutral_Unspecified       0.45      0.63      0.53       207\n","         Sadness_High       0.25      0.05      0.08        21\n","          Sadness_Low       0.14      0.06      0.09        16\n","       Sadness_Medium       0.25      0.29      0.27        14\n","  Sadness_Unspecified       0.49      0.60      0.54       205\n","\n","             accuracy                           0.47      1489\n","            macro avg       0.32      0.24      0.25      1489\n","         weighted avg       0.45      0.47      0.45      1489\n","\n","Confusion Matrix:\n","[[ 10   0   1   3   0   0   0   0   2   0   0   1   0   0   0   1   0   0\n","    0   0   0]\n"," [  0   1   1   2   0   0   0   4   1   0   0   1   1   2   0   4   3   0\n","    0   0   1]\n"," [  0   2   1   9   0   0   1   2   1   0   0   1   0   0   0   3   1   0\n","    1   0   0]\n"," [  0   0   0 141   0   0   0  12   0   0   0   4   0   0   0  19   5   0\n","    0   0   0]\n"," [  0   1   1   3   1   1   0   3   0   0   1   2   0   1   0   3   2   0\n","    0   0   0]\n"," [  0   0   1   0   1   0   1   1   0   1   0   1   0   0   0   1   4   0\n","    1   0   1]\n"," [  0   2   0   1   1   2   1   2   0   0   0   0   0   1   0   2   7   0\n","    0   0   1]\n"," [  0   0   0  17   0   1   0  75   0   0   0   9   0   0   0  24  26   0\n","    0   1  30]\n"," [  0   0   0   1   0   0   1   0   3   0   1   5   0   0   0   0   0   0\n","    0   0   0]\n"," [  0   0   0   0   0   1   0   0   0   1   0   4   0   0   0   1   3   0\n","    2   0   3]\n"," [  0   0   0   0   0   1   2   2   0   0   1   2   0   0   2   1   4   2\n","    0   2   3]\n"," [  0   0   0  17   0   0   0  15   1   0   0  82   0   0   0  36  22   0\n","    0   0  37]\n"," [  1   0   3   8   0   0   1   2   3   0   0   1   1   0   0   3   1   0\n","    0   0   0]\n"," [  0   0   0   1   1   0   0   1   0   1   1   0   0   1   0   4   6   0\n","    0   0   1]\n"," [  0   0   0   2   2   0   2   2   0   0   0   0   0   1   1   6   2   0\n","    0   0   1]\n"," [  0   0   0  33   0   0   0  15   0   0   0  20   0   0   0 128  31   0\n","    0   0   4]\n"," [  0   0   0   2   0   0   0  21   0   0   0  12   0   1   0  11 131   0\n","    0   0  29]\n"," [  0   0   0   0   0   1   0   1   0   1   0   1   0   0   0   0   4   1\n","    0   5   7]\n"," [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   3   0\n","    1   4   7]\n"," [  0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   2   0\n","    1   4   4]\n"," [  0   0   0   5   0   0   0  20   0   1   0  20   0   0   0   2  33   1\n","    1   0 122]]\n","audio1.mp3 downloaded\n","audio2.mp3 downloaded\n","  emotion        level  confidence\n","0   Anger  Unspecified        0.82\n","1   Anger  Unspecified        0.96\n"]}]}]}