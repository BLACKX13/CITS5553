{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"eac08a374cda4d0ca322d877a0254ef3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1128d8564442441181850ec63c4aad29","IPY_MODEL_45e7ecd08d1048f4a267f7a1ffccced2","IPY_MODEL_ebeca88217cf4c1d9f86daacf1baebf9"],"layout":"IPY_MODEL_ce3308ce76654aed84cd27a194759c31"}},"1128d8564442441181850ec63c4aad29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94cca2809d814c6688bf61ee55650551","placeholder":"​","style":"IPY_MODEL_421eb982f35f4500b852b08c5541dad1","value":"Casting the dataset: 100%"}},"45e7ecd08d1048f4a267f7a1ffccced2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa3910a4bf2d4af1a3432cdc9b7589d6","max":1152,"min":0,"orientation":"horizontal","style":"IPY_MODEL_668d93077efd4aa181e3a3adfa277d65","value":1152}},"ebeca88217cf4c1d9f86daacf1baebf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e128204069154ec68edcdfc52435c4de","placeholder":"​","style":"IPY_MODEL_72b1b5cd82ce40b282d895e82360971c","value":" 1152/1152 [00:00&lt;00:00, 25925.22 examples/s]"}},"ce3308ce76654aed84cd27a194759c31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94cca2809d814c6688bf61ee55650551":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"421eb982f35f4500b852b08c5541dad1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa3910a4bf2d4af1a3432cdc9b7589d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"668d93077efd4aa181e3a3adfa277d65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e128204069154ec68edcdfc52435c4de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72b1b5cd82ce40b282d895e82360971c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2716dac58dbf4385b6f0f5476d8099b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8abe8821e7234be1bc5cedf34a353958","IPY_MODEL_1ab3bc8ca4fb4753ad581daf2b4d1e34","IPY_MODEL_f5c737a4d52b4b2cb9271885b821d887"],"layout":"IPY_MODEL_caae48d50ad347b9a58b456b1965ab8d"}},"8abe8821e7234be1bc5cedf34a353958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73c2504e178c4cf9bdddfd61542c235b","placeholder":"​","style":"IPY_MODEL_78a1d038ae2a4326a693c6fc764a9fc6","value":"Casting the dataset: 100%"}},"1ab3bc8ca4fb4753ad581daf2b4d1e34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15d360719d7e41f485a31451e18969c1","max":288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86d2699a61614c859fd46004de0ccfc6","value":288}},"f5c737a4d52b4b2cb9271885b821d887":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49778efb1db24741a95323a808925849","placeholder":"​","style":"IPY_MODEL_a12f96b05bf84f0ca9984ca95916f114","value":" 288/288 [00:00&lt;00:00, 5169.13 examples/s]"}},"caae48d50ad347b9a58b456b1965ab8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c2504e178c4cf9bdddfd61542c235b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78a1d038ae2a4326a693c6fc764a9fc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15d360719d7e41f485a31451e18969c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86d2699a61614c859fd46004de0ccfc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49778efb1db24741a95323a808925849":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a12f96b05bf84f0ca9984ca95916f114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"806a5a0ff0054573a20d3dfcc6d45a77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d97510fe91c346a4b22fe735ffa530cd","IPY_MODEL_43fab57a9e134784938c60148cd831b3","IPY_MODEL_676cc187a52d4da59d90d406c6966364"],"layout":"IPY_MODEL_805b2f9783864f1aaeabb4c36e56642f"}},"d97510fe91c346a4b22fe735ffa530cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9b7fee114894dddb4c6261ef0db9ee3","placeholder":"​","style":"IPY_MODEL_5a9bccb6b95f4562ab0cdcaab7e58efe","value":"Map: 100%"}},"43fab57a9e134784938c60148cd831b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fd8fa4a49744a598e204249798ddee8","max":1152,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18dd4eb88ec047dfb5732829296e6e11","value":1152}},"676cc187a52d4da59d90d406c6966364":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2273952d0e4553abc8188d3ff4c2a9","placeholder":"​","style":"IPY_MODEL_a39243d437eb4d00bdda00a6ac9d1176","value":" 1152/1152 [00:13&lt;00:00, 115.22 examples/s]"}},"805b2f9783864f1aaeabb4c36e56642f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9b7fee114894dddb4c6261ef0db9ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a9bccb6b95f4562ab0cdcaab7e58efe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fd8fa4a49744a598e204249798ddee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18dd4eb88ec047dfb5732829296e6e11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff2273952d0e4553abc8188d3ff4c2a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a39243d437eb4d00bdda00a6ac9d1176":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6edf93919d3944c992a31e79954408a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6dcf4170d74c4335bb377db1a175cc12","IPY_MODEL_03790f7a316f48219030008156b54c7a","IPY_MODEL_3d0f689afd0e4f6285dd7ee2b7dc496e"],"layout":"IPY_MODEL_8d1ed8b0c9904452ab2d0c7474b9b295"}},"6dcf4170d74c4335bb377db1a175cc12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dd9ba08c524488aa24fa4ddc3fc3edd","placeholder":"​","style":"IPY_MODEL_248fb436b93e44c8a62fd0d247cf752e","value":"Map: 100%"}},"03790f7a316f48219030008156b54c7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c4b9007daaf40f7bc16401ce0574aa8","max":288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67fe6977ca4f4dc9bb66131dd562762c","value":288}},"3d0f689afd0e4f6285dd7ee2b7dc496e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ce02dd6040418786cc826d498c1612","placeholder":"​","style":"IPY_MODEL_917e2abcf96a4c29a2c1067123270ca6","value":" 288/288 [00:01&lt;00:00, 296.35 examples/s]"}},"8d1ed8b0c9904452ab2d0c7474b9b295":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dd9ba08c524488aa24fa4ddc3fc3edd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"248fb436b93e44c8a62fd0d247cf752e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c4b9007daaf40f7bc16401ce0574aa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67fe6977ca4f4dc9bb66131dd562762c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41ce02dd6040418786cc826d498c1612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"917e2abcf96a4c29a2c1067123270ca6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"S1C-K5MjNVe0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727348658578,"user_tz":-480,"elapsed":6577,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}},"outputId":"526a330e-8d29-469b-e8cf-989b7f1dba09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.4.1+cu121)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchaudio) (3.1.4)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n"]}],"source":["# Install necessary libraries\n","!pip install transformers datasets torchaudio librosa scikit-learn pandas"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","import librosa\n","from transformers import Wav2Vec2FeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\n","from datasets import Dataset, DatasetDict, Features, ClassLabel, Audio\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from zipfile import ZipFile\n","import shutil"],"metadata":{"id":"6e2Z4UJBNavG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727348673835,"user_tz":-480,"elapsed":15259,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}},"outputId":"6481d082-ffbf-4067-a4cc-268a2c938031"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"]}]},{"cell_type":"code","source":["class RAVDESSDatasetLoader:\n","    \"\"\"\n","    Handles downloading, extracting, loading, and preprocessing the RAVDESS dataset (emotion labels only).\n","    \"\"\"\n","    def __init__(self, dataset_zip_name='Audio_Speech_Actors_01-24.zip', dataset_folder='RAVDESS'):\n","        self.dataset_zip_name = dataset_zip_name\n","        self.dataset_folder = dataset_folder\n","        self.extract_path = f'./{self.dataset_folder}'\n","        self.emotion_mapping = {\n","            '01': 'neutral',\n","            '02': 'calm',\n","            '03': 'happy',\n","            '04': 'sad',\n","            '05': 'angry',\n","            '06': 'fearful',\n","            '07': 'disgust',\n","            '08': 'surprised'\n","        }\n","\n","    def authenticate_and_create_drive(self):\n","        \"\"\"\n","        Authenticates the user and creates a PyDrive GoogleDrive instance.\n","        \"\"\"\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","        return drive\n","\n","    def download_dataset(self, drive, file_id):\n","        \"\"\"\n","        Downloads the RAVDESS dataset zip file from Google Drive using its file ID.\n","        \"\"\"\n","        print(f\"Downloading dataset from Google Drive with file ID: {file_id}\")\n","        downloaded = drive.CreateFile({'id': file_id})\n","        downloaded.GetContentFile(self.dataset_zip_name)\n","        print(f\"Downloaded dataset as {self.dataset_zip_name}\")\n","\n","    def extract_dataset(self):\n","        \"\"\"\n","        Extracts the downloaded zip file to the specified directory.\n","        \"\"\"\n","        if not os.path.exists(self.extract_path):\n","            print(\"Extracting RAVDESS dataset...\")\n","            with ZipFile(self.dataset_zip_name, 'r') as zip_ref:\n","                zip_ref.extractall(self.extract_path)\n","            print(\"Extraction completed.\")\n","        else:\n","            print(\"RAVDESS dataset already extracted.\")\n","\n","    def load_data(self):\n","        \"\"\"\n","        Loads the RAVDESS dataset, parses filenames to extract emotion labels.\n","        \"\"\"\n","        data = []\n","        for root, dirs, files in os.walk(self.extract_path):\n","            for file in files:\n","                if file.endswith('.wav'):\n","                    filepath = os.path.join(root, file)\n","                    filename = os.path.basename(file)\n","                    parts = filename.split('.')[0].split('-')\n","                    if len(parts) != 7:\n","                        continue  # Skip files that don't match the naming convention\n","                    emotion_code = parts[2]\n","\n","                    emotion = self.emotion_mapping.get(emotion_code, 'unknown')\n","\n","                    if emotion == 'unknown':\n","                        continue\n","\n","                    label = f\"{emotion}\"  # Only emotion as the label\n","                    data.append({'file_path': filepath, 'label': label})\n","\n","        df = pd.DataFrame(data)\n","        print(f\"Total samples after filtering: {len(df)}\")\n","        return df\n","\n","    def prepare_datasets(self, df):\n","        \"\"\"\n","        Splits the DataFrame into training and testing sets and converts them into Hugging Face Datasets.\n","        \"\"\"\n","        train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n","        print(f\"Training samples: {len(train_df)}\")\n","        print(f\"Testing samples: {len(test_df)}\")\n","\n","        labels = sorted(df['label'].unique())\n","        num_labels = len(labels)\n","        label2id = {label: idx for idx, label in enumerate(labels)}\n","        id2label = {idx: label for label, idx in label2id.items()}\n","\n","        features = Features({\n","            'file_path': Audio(sampling_rate=16000),\n","            'label': ClassLabel(names=labels)\n","        })\n","\n","        train_dataset = Dataset.from_pandas(train_df).remove_columns('__index_level_0__').cast(features)\n","        test_dataset = Dataset.from_pandas(test_df).remove_columns('__index_level_0__').cast(features)\n","\n","        dataset = DatasetDict({\n","            'train': train_dataset,\n","            'test': test_dataset\n","        })\n","\n","        return dataset, label2id, id2label\n","\n","    def run(self, dataset_file_id):\n","        \"\"\"\n","        Executes the dataset loading and preparation steps.\n","        \"\"\"\n","        drive = self.authenticate_and_create_drive()\n","        self.download_dataset(drive, dataset_file_id)\n","        self.extract_dataset()\n","        df = self.load_data()\n","        dataset, label2id, id2label = self.prepare_datasets(df)\n","        return dataset, label2id, id2label\n","\n","\n","class EmotionIntensityModel:\n","    \"\"\"\n","    Manages the loading, fine-tuning, and evaluation of the DistilHuBERT model for emotion classification.\n","    \"\"\"\n","    def __init__(self, model_name, num_labels, label2id, id2label):\n","        self.model_name = model_name\n","        self.num_labels = num_labels\n","        self.label2id = label2id\n","        self.id2label = id2label\n","        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n","        self.model = self.initialize_model()\n","\n","    def initialize_model(self):\n","        \"\"\"\n","        Loads the pre-trained DistilHuBERT model and configures it for classification.\n","        \"\"\"\n","        model = AutoModelForAudioClassification.from_pretrained(\n","            self.model_name,\n","            num_labels=self.num_labels,\n","            label2id=self.label2id,\n","            id2label=self.id2label\n","        )\n","        return model\n","\n","    def preprocess_function(self, examples):\n","        \"\"\"\n","        Preprocesses the dataset by using the audio features provided in the dataset.\n","        \"\"\"\n","        audio = examples['file_path']\n","        inputs = self.feature_extractor(\n","            audio['array'],\n","            sampling_rate=audio['sampling_rate'],\n","            return_tensors=\"pt\",\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=61395\n","        )\n","        examples[\"input_values\"] = inputs[\"input_values\"][0].numpy()\n","        return examples\n","\n","    def prepare_data(self, dataset):\n","        \"\"\"\n","        Applies preprocessing to the dataset.\n","        \"\"\"\n","        dataset = dataset.map(self.preprocess_function, remove_columns=['file_path'])\n","        return dataset\n","\n","    def compute_metrics(self, pred):\n","        \"\"\"\n","        Computes evaluation metrics.\n","        \"\"\"\n","        labels = pred.label_ids\n","        preds = pred.predictions.argmax(-1)\n","        accuracy = accuracy_score(labels, preds)\n","        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n","        return {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","        }\n","\n","    def fine_tune(self, dataset, training_args):\n","        \"\"\"\n","        Fine-tunes the model using the Trainer API.\n","        \"\"\"\n","        trainer = Trainer(\n","            model=self.model,\n","            args=training_args,\n","            train_dataset=dataset[\"train\"],\n","            eval_dataset=dataset[\"test\"],\n","            tokenizer=None,\n","            compute_metrics=self.compute_metrics,\n","        )\n","\n","        trainer.train()\n","        return trainer\n","\n","    def evaluate(self, trainer, dataset):\n","        \"\"\"\n","        Evaluates the fine-tuned model on the test set.\n","        \"\"\"\n","        eval_results = trainer.evaluate()\n","        print(f\"Evaluation Results: {eval_results}\")\n","        return eval_results\n","\n","    def generate_reports(self, trainer, dataset, id2label):\n","        \"\"\"\n","        Generates classification reports and confusion matrices.\n","        \"\"\"\n","        predictions = trainer.predict(dataset[\"test\"])\n","        pred_ids = np.argmax(predictions.predictions, axis=1)\n","        true_ids = predictions.label_ids\n","\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(true_ids, pred_ids, target_names=list(id2label.values()), zero_division=0))\n","\n","        cm = confusion_matrix(true_ids, pred_ids)\n","        print(\"Confusion Matrix:\")\n","        print(cm)\n","\n","    def save_model(self, path=\"./fine_tuned_model\"):\n","        \"\"\"\n","        Saves the fine-tuned model and feature extractor.\n","        \"\"\"\n","        self.model.save_pretrained(path)\n","        self.feature_extractor.save_pretrained(path)\n","        print(f\"Model saved to {path}.\")\n","\n","    def load_model(self, path=\"./fine_tuned_model\"):\n","        \"\"\"\n","        Loads a previously fine-tuned model and feature extractor.\n","        \"\"\"\n","        self.model = AutoModelForAudioClassification.from_pretrained(path)\n","        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(path)\n","        print(f\"Model loaded from {path}.\")\n","\n","    def predict_all_labels(self, audio_file_path):\n","        \"\"\"\n","        Predict all possible emotion labels with their respective confidence scores.\n","        \"\"\"\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        speech_array, sampling_rate = librosa.load(audio_file_path, sr=16000)\n","        inputs = self.feature_extractor(\n","            speech_array, return_tensors=\"pt\", sampling_rate=sampling_rate, padding=True\n","        )\n","\n","        inputs = {key: value.to(device) for key, value in inputs.items()}\n","        self.model.to(device)\n","\n","        with torch.no_grad():\n","            logits = self.model(**inputs).logits\n","            probabilities = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()\n","\n","        id2label = self.model.config.id2label\n","\n","        results = []\n","        for label_id, confidence in enumerate(probabilities):\n","            emotion = id2label[label_id]\n","            results.append({\n","                'audio_file': os.path.basename(audio_file_path),\n","                'emotion': emotion.capitalize(),\n","                'confidence': confidence\n","            })\n","\n","        df = pd.DataFrame(results)\n","        df = df.sort_values(by='confidence', ascending=False).reset_index(drop=True)\n","\n","        return df\n","\n","class EmotionPipeline:\n","    \"\"\"\n","    Orchestrates the workflow of downloading data, fine-tuning the model, and evaluating its performance.\n","    \"\"\"\n","    def __init__(self, dataset_zip_file_id, model_path=\"./fine_tuned_model\", audio_train_switch=False):\n","        self.loader = RAVDESSDatasetLoader()\n","        self.model = None\n","        self.dataset_zip_file_id = dataset_zip_file_id\n","        self.audio_model_path = model_path  # Path where the model will be saved/loaded\n","        self.audio_train_switch = audio_train_switch  # Add a training switch to control model retraining\n","\n","    def run(self):\n","        \"\"\"\n","        Executes the entire pipeline: checks if the model exists, loads or fine-tunes it, and evaluates performance.\n","        \"\"\"\n","        dataset, label2id, id2label = self.loader.run(self.dataset_zip_file_id)\n","\n","        model_name = \"ntu-spml/distilhubert\"\n","\n","        # Check if the model is already saved locally or if training is forced by the switch\n","        if os.path.exists(self.audio_model_path) and not self.audio_train_switch:\n","            print(f\"Model found at {self.audio_model_path}. Loading the model...\")\n","            self.model = EmotionIntensityModel(model_name=model_name, num_labels=len(label2id), label2id=label2id, id2label=id2label)\n","            self.model.load_model(self.audio_model_path)\n","        else:\n","            if self.audio_train_switch:\n","                print(\"Audio training switch is ON. Re-training the model...\")\n","            else:\n","                print(\"No saved model found. Fine-tuning the model...\")\n","\n","            self.model = EmotionIntensityModel(model_name=model_name, num_labels=len(label2id), label2id=label2id, id2label=id2label)\n","\n","            dataset = self.model.prepare_data(dataset)\n","\n","            training_args = TrainingArguments(\n","                output_dir=\"./results\",\n","                evaluation_strategy=\"epoch\",\n","                save_strategy=\"epoch\",\n","                learning_rate=5e-5,\n","                per_device_train_batch_size=8,\n","                per_device_eval_batch_size=8,\n","                num_train_epochs=10,\n","                weight_decay=0.01,\n","                optim=\"adamw_torch\",\n","                logging_dir='./logs',\n","                logging_steps=10,\n","                lr_scheduler_type=\"linear\",\n","                warmup_ratio=0.1,\n","                seed=42,\n","                load_best_model_at_end=True,\n","                metric_for_best_model=\"accuracy\",\n","            )\n","\n","            trainer = self.model.fine_tune(dataset, training_args)\n","            self.model.evaluate(trainer, dataset)\n","            self.model.generate_reports(trainer, dataset, id2label)\n","\n","            # Save the fine-tuned model to the local directory\n","            self.model.save_model(self.audio_model_path)\n","\n","    def authenticate_and_create_drive(self):\n","        \"\"\"\n","        Authenticates the user and creates a PyDrive GoogleDrive instance.\n","        \"\"\"\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","        return drive\n","\n","    def download_audio_from_drive(self, drive, audio_file_id, destination_path):\n","        \"\"\"\n","        Downloads an audio file from Google Drive using its file ID.\n","        \"\"\"\n","        print(f\"Downloading audio file from Google Drive with file ID: {audio_file_id}\")\n","        downloaded = drive.CreateFile({'id': audio_file_id})\n","        downloaded.GetContentFile(destination_path)\n","        print(f\"Downloaded audio file and saved as {destination_path}\")\n","\n","    def load_and_predict(self, audio_file_ids):\n","        \"\"\"\n","        Downloads the audio files using their file IDs and predicts all possible labels.\n","        \"\"\"\n","        drive = self.authenticate_and_create_drive()\n","\n","        for audio_file_name, audio_file_id in audio_file_ids.items():\n","            destination_path = f\"./{audio_file_name}\"\n","            self.download_audio_from_drive(drive, audio_file_id, destination_path)\n","\n","            result_df = self.model.predict_all_labels(destination_path)\n","            print(result_df)"],"metadata":{"id":"9SGU9kMONch7","executionInfo":{"status":"ok","timestamp":1727348673836,"user_tz":-480,"elapsed":5,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def main():\n","    \"\"\"\n","    Main function to execute the emotion classification pipeline.\n","    \"\"\"\n","    dataset_zip_file_id = '1dSXSY-f6ZigkcJWV07v6kc3r_i3pMPvl'\n","    audio_model_path = './audio_emotion_model'\n","\n","    # Set audio_train_switch to True if you want to force re-training the model, otherwise False\n","    audio_train_switch = True\n","\n","    audio_pipeline = EmotionPipeline(dataset_zip_file_id=dataset_zip_file_id, model_path=audio_model_path, audio_train_switch=audio_train_switch)\n","    audio_pipeline.run()\n","\n","    audio_file_ids = {\n","        'audio1.mp3': '108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r'\n","    }\n","\n","    audio_pipeline.load_and_predict(audio_file_ids)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"BVBl7bRKNiAD","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["eac08a374cda4d0ca322d877a0254ef3","1128d8564442441181850ec63c4aad29","45e7ecd08d1048f4a267f7a1ffccced2","ebeca88217cf4c1d9f86daacf1baebf9","ce3308ce76654aed84cd27a194759c31","94cca2809d814c6688bf61ee55650551","421eb982f35f4500b852b08c5541dad1","aa3910a4bf2d4af1a3432cdc9b7589d6","668d93077efd4aa181e3a3adfa277d65","e128204069154ec68edcdfc52435c4de","72b1b5cd82ce40b282d895e82360971c","2716dac58dbf4385b6f0f5476d8099b5","8abe8821e7234be1bc5cedf34a353958","1ab3bc8ca4fb4753ad581daf2b4d1e34","f5c737a4d52b4b2cb9271885b821d887","caae48d50ad347b9a58b456b1965ab8d","73c2504e178c4cf9bdddfd61542c235b","78a1d038ae2a4326a693c6fc764a9fc6","15d360719d7e41f485a31451e18969c1","86d2699a61614c859fd46004de0ccfc6","49778efb1db24741a95323a808925849","a12f96b05bf84f0ca9984ca95916f114","806a5a0ff0054573a20d3dfcc6d45a77","d97510fe91c346a4b22fe735ffa530cd","43fab57a9e134784938c60148cd831b3","676cc187a52d4da59d90d406c6966364","805b2f9783864f1aaeabb4c36e56642f","e9b7fee114894dddb4c6261ef0db9ee3","5a9bccb6b95f4562ab0cdcaab7e58efe","3fd8fa4a49744a598e204249798ddee8","18dd4eb88ec047dfb5732829296e6e11","ff2273952d0e4553abc8188d3ff4c2a9","a39243d437eb4d00bdda00a6ac9d1176","6edf93919d3944c992a31e79954408a7","6dcf4170d74c4335bb377db1a175cc12","03790f7a316f48219030008156b54c7a","3d0f689afd0e4f6285dd7ee2b7dc496e","8d1ed8b0c9904452ab2d0c7474b9b295","4dd9ba08c524488aa24fa4ddc3fc3edd","248fb436b93e44c8a62fd0d247cf752e","4c4b9007daaf40f7bc16401ce0574aa8","67fe6977ca4f4dc9bb66131dd562762c","41ce02dd6040418786cc826d498c1612","917e2abcf96a4c29a2c1067123270ca6"]},"executionInfo":{"status":"ok","timestamp":1727349661307,"user_tz":-480,"elapsed":987475,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}},"outputId":"e2be1926-9197-48de-dd46-0ccb0f62599a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset from Google Drive with file ID: 1dSXSY-f6ZigkcJWV07v6kc3r_i3pMPvl\n","Downloaded dataset as Audio_Speech_Actors_01-24.zip\n","RAVDESS dataset already extracted.\n","Total samples after filtering: 1440\n","Training samples: 1152\n","Testing samples: 288\n"]},{"output_type":"display_data","data":{"text/plain":["Casting the dataset:   0%|          | 0/1152 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac08a374cda4d0ca322d877a0254ef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Casting the dataset:   0%|          | 0/288 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2716dac58dbf4385b6f0f5476d8099b5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Audio training switch is ON. Re-training the model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.bias', 'projector.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Parameter 'function'=<bound method EmotionIntensityModel.preprocess_function of <__main__.EmotionIntensityModel object at 0x7e0d24a6fca0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","WARNING:datasets.fingerprint:Parameter 'function'=<bound method EmotionIntensityModel.preprocess_function of <__main__.EmotionIntensityModel object at 0x7e0d24a6fca0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1152 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806a5a0ff0054573a20d3dfcc6d45a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/288 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6edf93919d3944c992a31e79954408a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1440/1440 15:09, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.881600</td>\n","      <td>1.772234</td>\n","      <td>0.315972</td>\n","      <td>0.188558</td>\n","      <td>0.315972</td>\n","      <td>0.218185</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.447000</td>\n","      <td>1.295342</td>\n","      <td>0.524306</td>\n","      <td>0.602552</td>\n","      <td>0.524306</td>\n","      <td>0.464297</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.017100</td>\n","      <td>0.951202</td>\n","      <td>0.687500</td>\n","      <td>0.717516</td>\n","      <td>0.687500</td>\n","      <td>0.662009</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.715400</td>\n","      <td>0.722277</td>\n","      <td>0.750000</td>\n","      <td>0.763750</td>\n","      <td>0.750000</td>\n","      <td>0.745656</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.353900</td>\n","      <td>0.610278</td>\n","      <td>0.770833</td>\n","      <td>0.774840</td>\n","      <td>0.770833</td>\n","      <td>0.760420</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.214700</td>\n","      <td>0.531886</td>\n","      <td>0.833333</td>\n","      <td>0.843677</td>\n","      <td>0.833333</td>\n","      <td>0.834087</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.085100</td>\n","      <td>0.517787</td>\n","      <td>0.840278</td>\n","      <td>0.845117</td>\n","      <td>0.840278</td>\n","      <td>0.838867</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.048600</td>\n","      <td>0.585231</td>\n","      <td>0.822917</td>\n","      <td>0.830888</td>\n","      <td>0.822917</td>\n","      <td>0.823221</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.021900</td>\n","      <td>0.558815</td>\n","      <td>0.836806</td>\n","      <td>0.846105</td>\n","      <td>0.836806</td>\n","      <td>0.836994</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.017500</td>\n","      <td>0.562754</td>\n","      <td>0.836806</td>\n","      <td>0.846465</td>\n","      <td>0.836806</td>\n","      <td>0.837378</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Results: {'eval_loss': 0.5177865624427795, 'eval_accuracy': 0.8402777777777778, 'eval_precision': 0.8451172578849229, 'eval_recall': 0.8402777777777778, 'eval_f1': 0.8388666594750462, 'eval_runtime': 13.4751, 'eval_samples_per_second': 21.373, 'eval_steps_per_second': 2.672, 'epoch': 10.0}\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","       angry       0.94      0.82      0.87        38\n","        calm       0.72      0.95      0.82        38\n","     disgust       0.88      0.92      0.90        38\n","     fearful       0.94      0.87      0.91        39\n","       happy       0.85      0.85      0.85        39\n","     neutral       0.67      0.53      0.59        19\n","         sad       0.82      0.74      0.78        38\n","   surprised       0.85      0.90      0.88        39\n","\n","    accuracy                           0.84       288\n","   macro avg       0.83      0.82      0.82       288\n","weighted avg       0.85      0.84      0.84       288\n","\n","Confusion Matrix:\n","[[31  0  5  0  2  0  0  0]\n"," [ 0 36  0  0  0  2  0  0]\n"," [ 1  0 35  0  0  0  1  1]\n"," [ 1  0  0 34  0  0  3  1]\n"," [ 0  0  0  0 33  1  2  3]\n"," [ 0  8  0  0  1 10  0  0]\n"," [ 0  6  0  1  0  2 28  1]\n"," [ 0  0  0  1  3  0  0 35]]\n","Model saved to ./audio_emotion_model.\n","Downloading audio file from Google Drive with file ID: 108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r\n","Downloaded audio file and saved as ./audio1.mp3\n","   audio_file    emotion  confidence\n","0  audio1.mp3  Surprised    0.948895\n","1  audio1.mp3    Disgust    0.023914\n","2  audio1.mp3      Happy    0.010683\n","3  audio1.mp3      Angry    0.010200\n","4  audio1.mp3    Neutral    0.003015\n","5  audio1.mp3    Fearful    0.001992\n","6  audio1.mp3        Sad    0.000697\n","7  audio1.mp3       Calm    0.000603\n"]}]}]}
