{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoLuoJKvwKUt","executionInfo":{"status":"ok","timestamp":1725896950544,"user_tz":-480,"elapsed":25230,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"305d3668-74a9-4062-f0e0-2c360d559097"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opensmile in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: audobject>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from opensmile) (0.7.11)\n","Requirement already satisfied: audinterface>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from opensmile) (1.2.2)\n","Requirement already satisfied: audeer>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (2.2.0)\n","Requirement already satisfied: audformat<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.0)\n","Requirement already satisfied: audiofile>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.5.0)\n","Requirement already satisfied: audmath>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.4.1)\n","Requirement already satisfied: audresample<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.3)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (8.0.0)\n","Requirement already satisfied: oyaml in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (24.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audeer>=1.18.0->audinterface>=0.7.0->opensmile) (4.66.4)\n","Requirement already satisfied: iso-639 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (0.4.5)\n","Requirement already satisfied: iso3166 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.1.1)\n","Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (14.0.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.25.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.19.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Git LFS initialized.\n","fatal: destination path 'CREMA-D' already exists and is not an empty directory.\n"]}],"source":["!pip install opensmile\n","!pip install --upgrade pandas\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://github.com/CheyneyComputerScience/CREMA-D.git"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"w82tug5zmA9i","executionInfo":{"status":"ok","timestamp":1725896950545,"user_tz":-480,"elapsed":6,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import opensmile\n","import audiofile\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"B4ktMNx-h4El","executionInfo":{"status":"ok","timestamp":1725896950545,"user_tz":-480,"elapsed":6,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["# AudioProcessor: Handles loading of audio files\n","class AudioProcessor:\n","    \"\"\"\n","    AudioProcessor handles loading of audio files.\n","    It extracts audio signals and sampling rates from audio files.\n","    \"\"\"\n","\n","    def __init__(self, file_paths):\n","        self.file_paths = file_paths  # List of audio file paths\n","\n","    def load_audio(self, path):\n","        \"\"\"\n","        Loads an audio file and returns the signal and sampling rate.\n","        \"\"\"\n","        try:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","        except Exception as e:\n","            print(f\"Error loading {path}: {str(e)}\")\n","            return None, None\n","        return signal, sampling_rate\n","\n","    def batch_load(self):\n","        \"\"\"\n","        Loads all audio files in batch.\n","        Returns a list of tuples containing the audio signals and sampling rates.\n","        \"\"\"\n","        signals = []\n","        for path in self.file_paths:\n","            signal, sampling_rate = self.load_audio(path)\n","            if signal is not None:\n","                signals.append((signal, sampling_rate))\n","        return signals"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"c060MFbokyZ4","executionInfo":{"status":"ok","timestamp":1725896950546,"user_tz":-480,"elapsed":7,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["# FeatureExtractor: Uses OpenSmile to extract features from audio\n","class FeatureExtractor:\n","    \"\"\"\n","    Extracts features from audio files using OpenSmile.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.smile = opensmile.Smile(\n","            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n","            feature_level=opensmile.FeatureLevel.Functionals\n","        )\n","\n","    def extract_features(self, signal, sampling_rate):\n","        \"\"\"\n","        Extracts features from a single audio signal using OpenSmile.\n","        \"\"\"\n","        features = self.smile.process_signal(signal, sampling_rate)\n","        return features\n","\n","    def extract_batch_from_paths(self, paths):\n","        \"\"\"\n","        Extracts features from a list of audio file paths.\n","        \"\"\"\n","        all_features = []\n","        for path in paths:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","            if signal is not None:\n","                features = self.extract_features(signal, sampling_rate)\n","                all_features.append(features)\n","        return pd.concat(all_features, ignore_index=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"2DQHMKrBk2OF","executionInfo":{"status":"ok","timestamp":1725896950546,"user_tz":-480,"elapsed":7,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["# EmotionClassifier: Random Forest classifier for emotion classification\n","class EmotionClassifier:\n","    \"\"\"\n","    A classifier for predicting emotions using a Random Forest model.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.model = RandomForestClassifier(random_state=42)\n","        self.label_encoder = LabelEncoder()\n","        self.scaler = StandardScaler()\n","\n","    def train(self, X_train, y_train):\n","        \"\"\"\n","        Trains the emotion classifier using scaled features and encoded labels.\n","        \"\"\"\n","        y_train_encoded = self.label_encoder.fit_transform(y_train)\n","        X_train_scaled = self.scaler.fit_transform(X_train)\n","        self.model.fit(X_train_scaled, y_train_encoded)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts emotions on new data and returns a list of PredictionResult objects.\n","        \"\"\"\n","        X_scaled = self.scaler.transform(X)\n","        y_pred_encoded = self.model.predict(X_scaled)\n","        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n","        y_proba = self.model.predict_proba(X_scaled)\n","        confidence_levels = y_proba.max(axis=1)\n","        return [PredictionResult(label, confidence) for label, confidence in zip(y_pred, confidence_levels)]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"w6djLkaamlyx","executionInfo":{"status":"ok","timestamp":1725896950546,"user_tz":-480,"elapsed":6,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["# PredictionResult: Stores emotion classification results\n","class PredictionResult:\n","    \"\"\"\n","    Stores the result of an emotion prediction.\n","    \"\"\"\n","\n","    def __init__(self, label, confidence):\n","        self.label = label  # Predicted emotion label\n","        self.confidence = round(confidence, 2)  # Confidence score\n","\n","    def __repr__(self):\n","        \"\"\"\n","        String representation of the prediction result.\n","        \"\"\"\n","        return f\"PredictionResult(label={self.label}, confidence={self.confidence:.2f})\""]},{"cell_type":"code","execution_count":15,"metadata":{"id":"1DHFd8OBmxwZ","executionInfo":{"status":"ok","timestamp":1725896950546,"user_tz":-480,"elapsed":6,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":["class AudioEmotionDetectionPipeline:\n","    \"\"\"\n","    Manages the workflow:\n","    - Extracts features using OpenSmile.\n","    - Trains a model using CREMA-D AudioMP3 files.\n","    - Predicts emotions on new audio files using the trained model.\n","    \"\"\"\n","\n","    def __init__(self, file_ids):\n","        self.file_ids = file_ids  # Google Drive audio file IDs\n","        self.processor = None  # To handle audio file processing\n","        self.extractor = FeatureExtractor()  # To extract features from audio\n","        self.classifier = EmotionClassifier()  # Emotion classifier\n","\n","    def load_crema_d_data(self):\n","        \"\"\"\n","        Loads CREMA-D AudioMP3 dataset, extracting file paths and emotion labels from filenames.\n","        Returns a DataFrame with file paths and labels.\n","        \"\"\"\n","        audio_dir = './CREMA-D/AudioMP3'\n","        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n","\n","        # Extract emotion labels from filenames\n","        emotions = {\n","            'ANG': 'Anger',\n","            'DIS': 'Disgust',\n","            'FEA': 'Fear',\n","            'HAP': 'Happiness',\n","            'NEU': 'Neutral',\n","            'SAD': 'Sadness'\n","        }\n","\n","        file_paths = []\n","        labels = []\n","\n","        for file in audio_files:\n","            emotion_code = file.split('_')[2]  # The third part of the filename contains the emotion code\n","            if emotion_code in emotions:\n","                file_paths.append(os.path.join(audio_dir, file))\n","                labels.append(emotions[emotion_code])\n","\n","        return pd.DataFrame({'Path': file_paths, 'Label': labels})\n","\n","    def download_and_extract_features(self):\n","        \"\"\"\n","        Downloads audio files from Google Drive and extracts features.\n","        Returns a DataFrame with extracted features.\n","        \"\"\"\n","        file_paths = self.download_files_from_drive(self.file_ids)\n","        self.processor = AudioProcessor(file_paths)\n","        return self.extractor.extract_batch_from_paths(file_paths)\n","\n","    def download_files_from_drive(self, file_ids):\n","        \"\"\"\n","        Downloads files from Google Drive using file IDs.\n","        Returns a list of file paths.\n","        \"\"\"\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","\n","        file_paths = []\n","        for filename, file_id in file_ids.items():\n","            downloaded = drive.CreateFile({'id': file_id})\n","            downloaded.GetContentFile(filename)\n","            file_paths.append(filename)\n","            print(f\"{filename} downloaded\")\n","        return file_paths\n","\n","    def train_classifier(self):\n","        \"\"\"\n","        Trains the emotion classifier using CREMA-D dataset.\n","        \"\"\"\n","        crema_d_data = self.load_crema_d_data()\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            crema_d_data['Path'], crema_d_data['Label'], test_size=0.2, random_state=42)\n","\n","        # Extract features for training and testing\n","        X_train_features = self.extractor.extract_batch_from_paths(X_train)\n","        X_test_features = self.extractor.extract_batch_from_paths(X_test)\n","\n","        print(f\"Shape of training features: {X_train_features.shape}\")\n","        print(f\"Shape of testing features: {X_test_features.shape}\")\n","\n","        self.classifier.train(X_train_features, y_train)\n","\n","        # Evaluate model performance\n","        y_test_pred = self.classifier.predict(X_test_features)\n","        print(\"Model evaluation on test set:\")\n","        print(classification_report(y_test, [result.label for result in y_test_pred]))\n","\n","        cm = confusion_matrix(y_test, [result.label for result in y_test_pred])\n","        print(\"Confusion Matrix:\")\n","        print(cm)\n","\n","    def run(self):\n","        \"\"\"\n","        Runs the entire pipeline and returns predictions.\n","        \"\"\"\n","        # Train classifier and predict on new audio files\n","        self.train_classifier()\n","        audio_features = self.download_and_extract_features()\n","\n","        # Predict on new audio files\n","        predictions = self.classifier.predict(audio_features)\n","\n","        # Convert predictions to DataFrame\n","        results_df = pd.DataFrame([{\n","            \"label\": pred.label,\n","            \"confidence\": pred.confidence\n","        } for pred in predictions])\n","\n","        return results_df"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ayBKc8XxmoKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725898519645,"user_tz":-480,"elapsed":1569105,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"d86900da-c806-49a0-e36c-2528c2f368ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training features: (5953, 88)\n","Shape of testing features: (1489, 88)\n","Model evaluation on test set:\n","              precision    recall  f1-score   support\n","\n","       Anger       0.63      0.72      0.67       239\n","     Disgust       0.43      0.30      0.35       297\n","        Fear       0.54      0.38      0.45       238\n","   Happiness       0.48      0.46      0.47       254\n","     Neutral       0.43      0.61      0.50       201\n","     Sadness       0.52      0.63      0.57       260\n","\n","    accuracy                           0.51      1489\n","   macro avg       0.51      0.52      0.50      1489\n","weighted avg       0.50      0.51      0.50      1489\n","\n","Confusion Matrix:\n","[[172  15   8  38   5   1]\n"," [ 29  89  12  37  66  64]\n"," [ 17  19  90  37  24  51]\n"," [ 51  26  22 118  29   8]\n"," [  0  30  12  12 122  25]\n"," [  3  29  22   4  39 163]]\n","audio1.mp3 downloaded\n","audio2.mp3 downloaded\n","   label  confidence\n","0  Anger        0.56\n","1  Anger        0.76\n"]}],"source":["# Main function to run the pipeline\n","def main():\n","    \"\"\"\n","    Main function that runs the entire emotion recognition pipeline.\n","    \"\"\"\n","    # Define Google Drive file IDs (replace with actual file IDs)\n","    file_ids = {\n","        'audio1.mp3': '108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r',\n","        'audio2.mp3': '13O1hKhYl5Uzlb0mIadH5hv5t_zSud664'\n","    }\n","\n","    # Create and run the AudioEmotionDetectionPipeline\n","    pipeline = AudioEmotionDetectionPipeline(file_ids)\n","    results_df = pipeline.run()\n","\n","    # Output the results\n","    print(results_df)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"d6qQhilJvdyd","executionInfo":{"status":"ok","timestamp":1725898519645,"user_tz":-480,"elapsed":12,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"15kbu3HmldIQ6sXP4rGlHRd9bInxYneyy","authorship_tag":"ABX9TyPFvreVAue+VfBqE+YXlLdw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
