{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_jLerOQ83mg","executionInfo":{"status":"ok","timestamp":1725992950685,"user_tz":-480,"elapsed":484746,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}},"outputId":"f77cde4f-bf2a-4f7d-dbab-74a665416de8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opensmile\n","  Downloading opensmile-2.5.0-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n","Collecting audobject>=0.6.1 (from opensmile)\n","  Downloading audobject-0.7.11-py3-none-any.whl.metadata (2.6 kB)\n","Collecting audinterface>=0.7.0 (from opensmile)\n","  Downloading audinterface-1.2.2-py3-none-any.whl.metadata (4.1 kB)\n","Collecting audeer>=1.18.0 (from audinterface>=0.7.0->opensmile)\n","  Downloading audeer-2.2.0-py3-none-any.whl.metadata (4.1 kB)\n","Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n","  Downloading audformat-1.3.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n","  Downloading audiofile-1.5.0-py3-none-any.whl.metadata (4.9 kB)\n","Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n","  Downloading audmath-1.4.1-py3-none-any.whl.metadata (3.6 kB)\n","Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n","  Downloading audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (8.4.0)\n","Collecting oyaml (from audobject>=0.6.1->opensmile)\n","  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (24.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audeer>=1.18.0->audinterface>=0.7.0->opensmile) (4.66.5)\n","Collecting iso-639 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n","  Downloading iso-639-0.4.5.tar.gz (167 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.4/167.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n","  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.1.4)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (14.0.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.26.4)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.20.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.16.0)\n","Downloading opensmile-2.5.0-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.2/996.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audinterface-1.2.2-py3-none-any.whl (68 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audobject-0.7.11-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audeer-2.2.0-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audformat-1.3.0-py3-none-any.whl (151 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.3/151.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audiofile-1.5.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading audmath-1.4.1-py3-none-any.whl (23 kB)\n","Downloading audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n","Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n","Building wheels for collected packages: iso-639\n","  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=168840 sha256=27bf670960a47d35dd5475f745910015532884bc464039befa1680a8e0f63458\n","  Stored in directory: /root/.cache/pip/wheels/d8/78/cc/5478ca3b1c3f602eae6f8cdbd78f909c0a0bfa0bbcb5c7771f\n","Successfully built iso-639\n","Installing collected packages: iso-639, oyaml, iso3166, audresample, audmath, audeer, audobject, audiofile, audformat, audinterface, opensmile\n","Successfully installed audeer-2.2.0 audformat-1.3.0 audinterface-1.2.2 audiofile-1.5.0 audmath-1.4.1 audobject-0.7.11 audresample-1.3.3 iso-639-0.4.5 iso3166-2.1.1 opensmile-2.5.0 oyaml-1.0\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Collecting pandas\n","  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.1.4\n","    Uninstalling pandas-2.1.4:\n","      Successfully uninstalled pandas-2.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pandas-2.2.2\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Git LFS initialized.\n","Cloning into 'CREMA-D'...\n","remote: Enumerating objects: 22501, done.\u001b[K\n","remote: Counting objects: 100% (73/73), done.\u001b[K\n","remote: Compressing objects: 100% (60/60), done.\u001b[K\n","remote: Total 22501 (delta 22), reused 57 (delta 13), pack-reused 22428 (from 1)\u001b[K\n","Receiving objects: 100% (22501/22501), 14.82 MiB | 14.26 MiB/s, done.\n","Resolving deltas: 100% (75/75), done.\n","Updating files: 100% (22342/22342), done.\n","Filtering content: 100% (22326/22326), 3.42 GiB | 8.14 MiB/s, done.\n"]}],"source":["!pip install opensmile\n","!pip install --upgrade pandas\n","!pip install xgboost\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://github.com/CheyneyComputerScience/CREMA-D.git"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import opensmile\n","import audiofile\n","from xgboost import XGBClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9QQ03JU9ANT","executionInfo":{"status":"ok","timestamp":1725992955457,"user_tz":-480,"elapsed":4786,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}},"outputId":"a1851d29-5381-44ce-c806-d3596330ebe9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"]}]},{"cell_type":"code","source":["# AudioProcessor: Handles loading of audio files\n","class AudioProcessor:\n","    \"\"\"\n","    AudioProcessor handles loading of audio files.\n","    It extracts audio signals and sampling rates from audio files.\n","    \"\"\"\n","\n","    def __init__(self, file_paths):\n","        self.file_paths = file_paths  # List of audio file paths\n","\n","    def load_audio(self, path):\n","        \"\"\"\n","        Loads an audio file and returns the signal and sampling rate.\n","        \"\"\"\n","        try:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","        except Exception as e:\n","            print(f\"Error loading {path}: {str(e)}\")\n","            return None, None\n","        return signal, sampling_rate\n","\n","    def batch_load(self):\n","        \"\"\"\n","        Loads all audio files in batch.\n","        Returns a list of tuples containing the audio signals and sampling rates.\n","        \"\"\"\n","        signals = []\n","        for path in self.file_paths:\n","            signal, sampling_rate = self.load_audio(path)\n","            if signal is not None:\n","                signals.append((signal, sampling_rate))\n","        return signals\n","\n","\n","# FeatureExtractor: Uses OpenSmile to extract features from audio\n","class FeatureExtractor:\n","    \"\"\"\n","    Extracts features from audio files using OpenSmile.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.smile = opensmile.Smile(\n","            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n","            feature_level=opensmile.FeatureLevel.Functionals\n","        )\n","\n","    def extract_features(self, signal, sampling_rate):\n","        \"\"\"\n","        Extracts features from a single audio signal using OpenSmile.\n","        \"\"\"\n","        features = self.smile.process_signal(signal, sampling_rate)\n","        return features\n","\n","    def extract_batch_from_paths(self, paths):\n","        \"\"\"\n","        Extracts features from a list of audio file paths.\n","        \"\"\"\n","        all_features = []\n","        for path in paths:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","            if signal is not None:\n","                features = self.extract_features(signal, sampling_rate)\n","                all_features.append(features)\n","        return pd.concat(all_features, ignore_index=True)\n","\n","\n","# EmotionClassifier: XGBoost classifier with RandomizedSearchCV for hyperparameter tuning\n","class EmotionClassifier:\n","    \"\"\"\n","    A classifier for predicting emotions using XGBoost with RandomizedSearchCV for faster hyperparameter tuning.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.model = XGBClassifier(random_state=42)\n","        self.label_encoder = LabelEncoder()\n","        self.scaler = StandardScaler()\n","\n","    def train(self, X_train, y_train):\n","        \"\"\"\n","        Trains the emotion classifier using scaled features and encoded labels.\n","        Uses RandomizedSearchCV for more comprehensive hyperparameter tuning.\n","        \"\"\"\n","        y_train_encoded = self.label_encoder.fit_transform(y_train)\n","        X_train_scaled = self.scaler.fit_transform(X_train)\n","\n","        # Define a hyperparameter grid\n","        param_distributions = {\n","            'n_estimators': [100, 200],\n","            'max_depth': [3, 5],\n","            'learning_rate': [0.01, 0.05, 0.1]\n","        }\n","\n","        # Use RandomizedSearchCV with more iterations\n","        randomized_search = RandomizedSearchCV(\n","            estimator=self.model,\n","            param_distributions=param_distributions,\n","            n_iter=5,  # Test 5 parameter combinations\n","            cv=3,      # Use 3-fold cross-validation for better validation\n","            scoring='accuracy',\n","            verbose=2,\n","            n_jobs=-1  # Use all available cores for parallel processing\n","        )\n","        randomized_search.fit(X_train_scaled, y_train_encoded)\n","\n","        # Use the best model from RandomizedSearchCV\n","        self.model = randomized_search.best_estimator_\n","        print(f\"Best parameters found: {randomized_search.best_params_}\")\n","\n","        # Evaluate cross-validation scores\n","        cv_scores = cross_val_score(self.model, X_train_scaled, y_train_encoded, cv=3, scoring='accuracy')\n","        print(f\"Cross-validation scores: {cv_scores}\")\n","        print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts emotions on new data and returns a list of PredictionResult objects.\n","        \"\"\"\n","        X_scaled = self.scaler.transform(X)\n","        y_pred_encoded = self.model.predict(X_scaled)\n","        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n","        y_proba = self.model.predict_proba(X_scaled)\n","        confidence_levels = y_proba.max(axis=1)\n","        return [PredictionResult(label, confidence) for label, confidence in zip(y_pred, confidence_levels)]\n","\n","\n","# PredictionResult: Stores emotion classification results\n","class PredictionResult:\n","    \"\"\"\n","    Stores the result of an emotion prediction.\n","    \"\"\"\n","\n","    def __init__(self, label, confidence):\n","        self.label = label  # Predicted emotion label\n","        self.confidence = round(confidence, 2)  # Confidence score\n","\n","    def __repr__(self):\n","        \"\"\"\n","        String representation of the prediction result.\n","        \"\"\"\n","        return f\"PredictionResult(label={self.label}, confidence={self.confidence:.2f})\"\n","\n","\n","# AudioEmotionDetectionPipeline: Get results\n","class AudioEmotionDetectionPipeline:\n","    \"\"\"\n","    Manages the workflow:\n","    - Extracts features using OpenSmile.\n","    - Trains a model using CREMA-D AudioMP3 files.\n","    - Predicts emotions on new audio files using the trained model.\n","    \"\"\"\n","\n","    def __init__(self, file_ids):\n","        self.file_ids = file_ids  # Google Drive audio file IDs\n","        self.processor = None  # To handle audio file processing\n","        self.extractor = FeatureExtractor()  # To extract features from audio\n","        self.classifier = EmotionClassifier()  # Emotion classifier\n","\n","    def load_crema_d_data(self):\n","        \"\"\"\n","        Loads CREMA-D AudioMP3 dataset, extracting file paths and emotion labels from filenames.\n","        Returns a DataFrame with file paths and labels.\n","        \"\"\"\n","        audio_dir = './CREMA-D/AudioMP3'\n","        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n","\n","        # Extract emotion labels from filenames\n","        emotions = {\n","            'ANG': 'Anger',\n","            'DIS': 'Disgust',\n","            'FEA': 'Fear',\n","            'HAP': 'Happiness',\n","            'NEU': 'Neutral',\n","            'SAD': 'Sadness'\n","        }\n","\n","        file_paths = []\n","        labels = []\n","\n","        for file in audio_files:\n","            emotion_code = file.split('_')[2]  # The third part of the filename contains the emotion code\n","            if emotion_code in emotions:\n","                file_paths.append(os.path.join(audio_dir, file))\n","                labels.append(emotions[emotion_code])\n","\n","        return pd.DataFrame({'Path': file_paths, 'Label': labels})\n","\n","    def download_and_extract_features(self):\n","        \"\"\"\n","        Downloads audio files from Google Drive and extracts features.\n","        Returns a DataFrame with extracted features.\n","        \"\"\"\n","        file_paths = self.download_files_from_drive(self.file_ids)\n","        self.processor = AudioProcessor(file_paths)\n","        return self.extractor.extract_batch_from_paths(file_paths)\n","\n","    def download_files_from_drive(self, file_ids):\n","        \"\"\"\n","        Downloads files from Google Drive using file IDs.\n","        Returns a list of file paths.\n","        \"\"\"\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","\n","        file_paths = []\n","        for filename, file_id in file_ids.items():\n","            downloaded = drive.CreateFile({'id': file_id})\n","            downloaded.GetContentFile(filename)\n","            file_paths.append(filename)\n","            print(f\"{filename} downloaded\")\n","        return file_paths\n","\n","    def train_classifier(self):\n","        \"\"\"\n","        Trains the emotion classifier using CREMA-D dataset.\n","        \"\"\"\n","        crema_d_data = self.load_crema_d_data()\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            crema_d_data['Path'], crema_d_data['Label'], test_size=0.2, random_state=42)\n","\n","        # Extract features for training and testing\n","        X_train_features = self.extractor.extract_batch_from_paths(X_train)\n","        X_test_features = self.extractor.extract_batch_from_paths(X_test)\n","\n","        print(f\"Shape of training features: {X_train_features.shape}\")\n","        print(f\"Shape of testing features: {X_test_features.shape}\")\n","\n","        self.classifier.train(X_train_features, y_train)\n","\n","        # Evaluate model performance\n","        y_test_pred = self.classifier.predict(X_test_features)\n","        print(\"Model evaluation on test set:\")\n","        print(classification_report(y_test, [result.label for result in y_test_pred]))\n","\n","        cm = confusion_matrix(y_test, [result.label for result in y_test_pred])\n","        print(\"Confusion Matrix:\")\n","        print(cm)\n","\n","    def run(self):\n","        \"\"\"\n","        Runs the entire pipeline and returns predictions.\n","        \"\"\"\n","        # Train classifier and predict on new audio files\n","        self.train_classifier()\n","        audio_features = self.download_and_extract_features()\n","\n","        # Predict on new audio files\n","        predictions = self.classifier.predict(audio_features)\n","\n","        # Convert predictions to DataFrame\n","        results_df = pd.DataFrame([{\n","            \"label\": pred.label,\n","            \"confidence\": pred.confidence\n","        } for pred in predictions])\n","\n","        return results_df"],"metadata":{"id":"3Jr7mmwG9EDW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main function to run the pipeline\n","def main():\n","    \"\"\"\n","    Main function that runs the entire emotion recognition pipeline.\n","    \"\"\"\n","    # Define Google Drive file IDs (replace with actual file IDs)\n","    file_ids = {\n","        'audio1.mp3': '108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r',\n","        'audio2.mp3': '13O1hKhYl5Uzlb0mIadH5hv5t_zSud664'\n","    }\n","\n","    # Create and run the AudioEmotionDetectionPipeline\n","    pipeline = AudioEmotionDetectionPipeline(file_ids)\n","    results_df = pipeline.run()\n","\n","    # Output the results\n","    print(results_df)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alS6gLqO9kQ3","outputId":"f5a6e1da-b0a9-43c5-b28f-a253c01c22fe","executionInfo":{"status":"ok","timestamp":1725996870932,"user_tz":-480,"elapsed":3915478,"user":{"displayName":"Arc Lich","userId":"02724969290719725870"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training features: (5953, 88)\n","Shape of testing features: (1489, 88)\n","Fitting 3 folds for each of 5 candidates, totalling 15 fits\n","Best parameters found: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1}\n","Cross-validation scores: [0.5138539  0.5141129  0.52469758]\n","Mean cross-validation score: 0.5175547960510279\n","Model evaluation on test set:\n","              precision    recall  f1-score   support\n","\n","       Anger       0.66      0.71      0.68       242\n","     Disgust       0.44      0.40      0.42       235\n","        Fear       0.48      0.38      0.42       258\n","   Happiness       0.50      0.49      0.49       291\n","     Neutral       0.52      0.59      0.56       207\n","     Sadness       0.57      0.63      0.60       256\n","\n","    accuracy                           0.53      1489\n","   macro avg       0.53      0.53      0.53      1489\n","weighted avg       0.53      0.53      0.53      1489\n","\n","Confusion Matrix:\n","[[171  12   8  42   7   2]\n"," [ 23  94  23  30  26  39]\n"," [ 20  21  99  49  17  52]\n"," [ 44  35  41 143  25   3]\n"," [  1  28  11  18 123  26]\n"," [  1  26  26   5  37 161]]\n","audio1.mp3 downloaded\n","audio2.mp3 downloaded\n","   label  confidence\n","0  Anger        0.91\n","1  Anger        0.97\n"]}]}]}
