{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiE7b8PvGBQXAE2n9gXP3r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1whmQn4c914","executionInfo":{"status":"ok","timestamp":1726069254356,"user_tz":-480,"elapsed":29660,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"56f1644c-a881-49e7-e99b-66ac47194885"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opensmile in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: audobject>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from opensmile) (0.7.11)\n","Requirement already satisfied: audinterface>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from opensmile) (1.2.2)\n","Requirement already satisfied: audeer>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (2.2.0)\n","Requirement already satisfied: audformat<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.0)\n","Requirement already satisfied: audiofile>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.5.0)\n","Requirement already satisfied: audmath>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.4.1)\n","Requirement already satisfied: audresample<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from audinterface>=0.7.0->opensmile) (1.3.3)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (8.4.0)\n","Requirement already satisfied: oyaml in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from audobject>=0.6.1->opensmile) (24.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from audeer>=1.18.0->audinterface>=0.7.0->opensmile) (4.66.5)\n","Requirement already satisfied: iso-639 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (0.4.5)\n","Requirement already satisfied: iso3166 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.1.1)\n","Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (14.0.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.26.4)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.20.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2024.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.22.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Git LFS initialized.\n","fatal: destination path 'CREMA-D' already exists and is not an empty directory.\n"]}],"source":["!pip install opensmile\n","!pip install --upgrade pandas\n","!pip install xgboost\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://github.com/CheyneyComputerScience/CREMA-D.git"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import opensmile\n","import audiofile\n","from xgboost import XGBClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import cross_val_score\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKXc9v38dJXE","executionInfo":{"status":"ok","timestamp":1726069257321,"user_tz":-480,"elapsed":2967,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"ee247129-53cc-4d6b-bd28-3d0d12ae7116"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"]}]},{"cell_type":"code","source":["# AudioProcessor: Handles loading of audio files\n","class AudioProcessor:\n","    \"\"\"\n","    AudioProcessor handles loading of audio files.\n","    It extracts audio signals and sampling rates from audio files.\n","    \"\"\"\n","\n","    def __init__(self, file_paths):\n","        self.file_paths = file_paths  # List of audio file paths\n","\n","    def load_audio(self, path):\n","        \"\"\"\n","        Loads an audio file and returns the signal and sampling rate.\n","        \"\"\"\n","        try:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","        except Exception as e:\n","            print(f\"Error loading {path}: {str(e)}\")\n","            return None, None\n","        return signal, sampling_rate\n","\n","    def batch_load(self):\n","        \"\"\"\n","        Loads all audio files in batch.\n","        Returns a list of tuples containing the audio signals and sampling rates.\n","        \"\"\"\n","        signals = []\n","        for path in self.file_paths:\n","            signal, sampling_rate = self.load_audio(path)\n","            if signal is not None:\n","                signals.append((signal, sampling_rate))\n","        return signals\n","\n","\n","# FeatureExtractor: Uses OpenSmile to extract features from audio\n","class FeatureExtractor:\n","    \"\"\"\n","    Extracts features from audio files using OpenSmile.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.smile = opensmile.Smile(\n","            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n","            feature_level=opensmile.FeatureLevel.Functionals\n","        )\n","\n","    def extract_features(self, signal, sampling_rate):\n","        \"\"\"\n","        Extracts features from a single audio signal using OpenSmile.\n","        \"\"\"\n","        features = self.smile.process_signal(signal, sampling_rate)\n","        return features\n","\n","    def extract_batch_from_paths(self, paths):\n","        \"\"\"\n","        Extracts features from a list of audio file paths.\n","        \"\"\"\n","        all_features = []\n","        for path in paths:\n","            signal, sampling_rate = audiofile.read(path, always_2d=True)\n","            if signal is not None:\n","                features = self.extract_features(signal, sampling_rate)\n","                all_features.append(features)\n","        return pd.concat(all_features, ignore_index=True)\n","\n","\n","# EmotionClassifier: XGBoost classifier with RandomizedSearchCV for hyperparameter tuning\n","class EmotionClassifier:\n","    \"\"\"\n","    A classifier for predicting emotions using XGBoost with RandomizedSearchCV for faster hyperparameter tuning.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.model = XGBClassifier(random_state=42)\n","        self.label_encoder = LabelEncoder()\n","        self.scaler = StandardScaler()\n","\n","    def train(self, X_train, y_train):\n","        \"\"\"\n","        Trains the emotion classifier using scaled features and encoded labels.\n","        Uses RandomizedSearchCV for more comprehensive hyperparameter tuning.\n","        \"\"\"\n","        y_train_encoded = self.label_encoder.fit_transform(y_train)\n","        X_train_scaled = self.scaler.fit_transform(X_train)\n","\n","        # Define a hyperparameter grid\n","        param_distributions = {\n","            'n_estimators': [100, 200],\n","            'max_depth': [3, 5],\n","            'learning_rate': [0.01, 0.05, 0.1]\n","        }\n","\n","        # Use RandomizedSearchCV with more iterations\n","        randomized_search = RandomizedSearchCV(\n","            estimator=self.model,\n","            param_distributions=param_distributions,\n","            n_iter=5,\n","            cv=3,\n","            scoring='accuracy',\n","            verbose=2,\n","            n_jobs=-1\n","        )\n","        randomized_search.fit(X_train_scaled, y_train_encoded)\n","\n","        # Use the best model from RandomizedSearchCV\n","        self.model = randomized_search.best_estimator_\n","        print(f\"Best parameters found: {randomized_search.best_params_}\")\n","\n","        # Evaluate cross-validation scores\n","        cv_scores = cross_val_score(self.model, X_train_scaled, y_train_encoded, cv=3, scoring='accuracy')\n","        print(f\"Cross-validation scores: {cv_scores}\")\n","        print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts emotions on new data and returns a list of all possible PredictionResult objects.\n","        \"\"\"\n","        X_scaled = self.scaler.transform(X)\n","        y_proba = self.model.predict_proba(X_scaled)\n","        y_classes = self.label_encoder.classes_\n","\n","        # Create a list of all emotions, levels, and their corresponding confidence scores\n","        all_predictions = []\n","        for i in range(len(X)):\n","            sorted_indices = np.argsort(-y_proba[i])  # Sort by probability in descending order\n","            predictions_for_sample = []\n","            for idx in sorted_indices:\n","                emotion = y_classes[idx]\n","                prob = y_proba[i][idx]\n","                predictions_for_sample.append(PredictionResult(emotion, prob))\n","            all_predictions.append(predictions_for_sample)\n","\n","        return all_predictions\n","\n","    def predict_top_label(self, X):\n","        \"\"\"\n","        Predicts the top emotion label for each sample.\n","        \"\"\"\n","        X_scaled = self.scaler.transform(X)\n","        y_pred_encoded = self.model.predict(X_scaled)\n","        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n","        return y_pred\n","\n","\n","# PredictionResult: Stores emotion classification results\n","class PredictionResult:\n","    \"\"\"\n","    Stores the result of an emotion prediction.\n","    \"\"\"\n","\n","    def __init__(self, label, confidence):\n","        self.label = label  # Predicted emotion label\n","        self.confidence = round(confidence, 2)  # Confidence score\n","\n","    def __repr__(self):\n","        \"\"\"\n","        String representation of the prediction result.\n","        \"\"\"\n","        return f\"PredictionResult(label={self.label}, confidence={self.confidence:.2f})\"\n","\n","\n","# AudioEmotionDetectionPipeline: Get results\n","class AudioEmotionDetectionPipeline:\n","    \"\"\"\n","    Manages the workflow:\n","    - Extracts features using OpenSmile.\n","    - Trains a model using CREMA-D AudioMP3 files.\n","    - Predicts emotions on new audio files using the trained model.\n","    \"\"\"\n","\n","    def __init__(self, file_ids):\n","        self.file_ids = file_ids  # Google Drive audio file IDs\n","        self.processor = None  # To handle audio file processing\n","        self.extractor = FeatureExtractor()  # To extract features from audio\n","        self.classifier = EmotionClassifier()  # Emotion classifier\n","\n","    def load_crema_d_data(self):\n","        \"\"\"\n","        Loads CREMA-D AudioMP3 dataset, extracting file paths, emotion labels, and emotion levels from filenames.\n","        Returns a DataFrame with file paths, combined emotion labels and levels.\n","        \"\"\"\n","        audio_dir = './CREMA-D/AudioMP3'\n","        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.mp3')]\n","\n","        # Define emotion and level mappings\n","        emotions = {\n","            'ANG': 'Anger',\n","            'DIS': 'Disgust',\n","            'FEA': 'Fear',\n","            'HAP': 'Happiness',\n","            'NEU': 'Neutral',\n","            'SAD': 'Sadness'\n","        }\n","\n","        levels = {\n","            'LO': 'Low',\n","            'MD': 'Medium',\n","            'HI': 'High',\n","            'XX': 'Unspecified'\n","        }\n","\n","        file_paths = []\n","        labels = []\n","\n","        for file in audio_files:\n","            parts = file.split('_')\n","\n","            if len(parts) >= 4:\n","                emotion_code = parts[2]  # The third part is the emotion\n","                level_code = parts[3].replace('.mp3', '')  # Remove the .mp3 extension\n","\n","                if emotion_code in emotions and level_code in levels:\n","                    emotion = emotions[emotion_code]\n","                    level = levels[level_code]\n","                    combined_label = f\"{emotion}_{level}\"  # Combine emotion and level\n","\n","                    file_paths.append(os.path.join(audio_dir, file))\n","                    labels.append(combined_label)\n","\n","        print(f\"Loaded {len(labels)} labels from the files.\")\n","\n","        return pd.DataFrame({'Path': file_paths, 'Label': labels})\n","\n","    def download_and_extract_features(self):\n","        \"\"\"\n","        Downloads audio files from Google Drive and extracts features.\n","        Returns features and a list of file paths.\n","        \"\"\"\n","        file_paths = self.download_files_from_drive(self.file_ids)\n","        self.processor = AudioProcessor(file_paths)\n","        features = self.extractor.extract_batch_from_paths(file_paths)\n","        return features, file_paths  # Return features and file_paths\n","\n","    def download_files_from_drive(self, file_ids):\n","        \"\"\"\n","        Downloads files from Google Drive using file IDs.\n","        Returns a list of file paths.\n","        \"\"\"\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","\n","        file_paths = []\n","        for filename, file_id in file_ids.items():\n","            downloaded = drive.CreateFile({'id': file_id})\n","            downloaded.GetContentFile(filename)\n","            file_paths.append(filename)\n","            print(f\"{filename} downloaded\")\n","        return file_paths\n","\n","    def train_classifier(self):\n","        \"\"\"\n","        Trains the emotion classifier using CREMA-D dataset.\n","        \"\"\"\n","        crema_d_data = self.load_crema_d_data()\n","\n","        # Debugging print: Check the size of the dataset before splitting\n","        print(f\"Dataset size before splitting: {crema_d_data.shape}\")\n","\n","        if crema_d_data.empty:\n","            print(\"Error: The dataset is empty!\")\n","            return\n","\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            crema_d_data['Path'], crema_d_data['Label'], test_size=0.2, random_state=42)\n","\n","        # Extract features for training and testing\n","        X_train_features = self.extractor.extract_batch_from_paths(X_train)\n","        X_test_features = self.extractor.extract_batch_from_paths(X_test)\n","\n","        print(f\"Shape of training features: {X_train_features.shape}\")\n","        print(f\"Shape of testing features: {X_test_features.shape}\")\n","\n","        self.classifier.train(X_train_features, y_train)\n","\n","        # Evaluate model performance\n","        y_test_pred = self.classifier.predict_top_label(X_test_features)\n","        print(\"Model evaluation on test set:\")\n","        print(classification_report(y_test, y_test_pred))\n","\n","        cm = confusion_matrix(y_test, y_test_pred)\n","        print(\"Confusion Matrix:\")\n","        print(cm)\n","\n","    def run(self):\n","        \"\"\"\n","        Runs the entire pipeline and returns predictions for multiple audio files.\n","        \"\"\"\n","        # Train classifier and predict on new audio files\n","        self.train_classifier()\n","        audio_features, file_paths = self.download_and_extract_features()\n","\n","        # Predict on new audio files\n","        all_predictions = self.classifier.predict(audio_features)\n","\n","        # Prepare DataFrame for all predictions with audio file reference\n","        results = []\n","        for i, sample_predictions in enumerate(all_predictions):\n","            # Get the corresponding audio file name for this sample\n","            audio_file = os.path.basename(file_paths[i])  # Get file name\n","            for pred in sample_predictions:\n","                emotion, level = pred.label.split('_')\n","                results.append({\n","                    \"audio_file\": audio_file,\n","                    \"emotion\": emotion,\n","                    \"level\": level,\n","                    \"confidence\": pred.confidence\n","                })\n","        return pd.DataFrame(results)"],"metadata":{"id":"zq_dT4MHdK4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main function to run the pipeline\n","def main():\n","  \"\"\"\n","  Main function that runs the entire emotion recognition pipeline.\n","  \"\"\"\n","  # Define Google Drive file IDs (replace with actual file IDs)\n","  file_ids = {\n","      'audio1.mp3': '108kPpEQeA_6RkQXmmLWDJXQzdiISlm0r',\n","      'audio2.mp3': '13O1hKhYl5Uzlb0mIadH5hv5t_zSud664'\n","  }\n","\n","  # Create and run the AudioEmotionDetectionPipeline\n","  pipeline = AudioEmotionDetectionPipeline(file_ids)\n","  results_df = pipeline.run()\n","\n","  # Output the results\n","  print(results_df)\n","\n","\n","if __name__ == \"__main__\":\n","  main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgiPEzgBeWIy","executionInfo":{"status":"ok","timestamp":1726072030924,"user_tz":-480,"elapsed":2773605,"user":{"displayName":"MINGYAO XIA","userId":"16248291646667849246"}},"outputId":"fb5efaa9-1c07-4203-cba9-6a2faeed1949"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 7442 labels from the files.\n","Dataset size before splitting: (7442, 2)\n","Shape of training features: (5953, 88)\n","Shape of testing features: (1489, 88)\n","Fitting 3 folds for each of 5 candidates, totalling 15 fits\n","Best parameters found: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05}\n","Cross-validation scores: [0.4534005  0.43447581 0.43951613]\n","Mean cross-validation score: 0.44246414642073617\n","Model evaluation on test set:\n","                       precision    recall  f1-score   support\n","\n","           Anger_High       0.91      0.56      0.69        18\n","            Anger_Low       0.17      0.05      0.07        21\n","         Anger_Medium       0.12      0.05      0.07        22\n","    Anger_Unspecified       0.58      0.78      0.66       181\n","         Disgust_High       0.17      0.05      0.08        19\n","          Disgust_Low       0.00      0.00      0.00        13\n","       Disgust_Medium       0.11      0.05      0.07        20\n","  Disgust_Unspecified       0.42      0.41      0.42       183\n","            Fear_High       0.27      0.27      0.27        11\n","             Fear_Low       0.17      0.07      0.10        15\n","          Fear_Medium       0.20      0.05      0.07        22\n","     Fear_Unspecified       0.49      0.39      0.43       210\n","       Happiness_High       0.50      0.04      0.08        24\n","        Happiness_Low       0.14      0.06      0.08        17\n","     Happiness_Medium       0.33      0.05      0.09        19\n","Happiness_Unspecified       0.51      0.55      0.53       231\n","  Neutral_Unspecified       0.45      0.63      0.53       207\n","         Sadness_High       0.25      0.05      0.08        21\n","          Sadness_Low       0.14      0.06      0.09        16\n","       Sadness_Medium       0.25      0.29      0.27        14\n","  Sadness_Unspecified       0.49      0.60      0.54       205\n","\n","             accuracy                           0.47      1489\n","            macro avg       0.32      0.24      0.25      1489\n","         weighted avg       0.45      0.47      0.45      1489\n","\n","Confusion Matrix:\n","[[ 10   0   1   3   0   0   0   0   2   0   0   1   0   0   0   1   0   0\n","    0   0   0]\n"," [  0   1   1   2   0   0   0   4   1   0   0   1   1   2   0   4   3   0\n","    0   0   1]\n"," [  0   2   1   9   0   0   1   2   1   0   0   1   0   0   0   3   1   0\n","    1   0   0]\n"," [  0   0   0 141   0   0   0  12   0   0   0   4   0   0   0  19   5   0\n","    0   0   0]\n"," [  0   1   1   3   1   1   0   3   0   0   1   2   0   1   0   3   2   0\n","    0   0   0]\n"," [  0   0   1   0   1   0   1   1   0   1   0   1   0   0   0   1   4   0\n","    1   0   1]\n"," [  0   2   0   1   1   2   1   2   0   0   0   0   0   1   0   2   7   0\n","    0   0   1]\n"," [  0   0   0  17   0   1   0  75   0   0   0   9   0   0   0  24  26   0\n","    0   1  30]\n"," [  0   0   0   1   0   0   1   0   3   0   1   5   0   0   0   0   0   0\n","    0   0   0]\n"," [  0   0   0   0   0   1   0   0   0   1   0   4   0   0   0   1   3   0\n","    2   0   3]\n"," [  0   0   0   0   0   1   2   2   0   0   1   2   0   0   2   1   4   2\n","    0   2   3]\n"," [  0   0   0  17   0   0   0  15   1   0   0  82   0   0   0  36  22   0\n","    0   0  37]\n"," [  1   0   3   8   0   0   1   2   3   0   0   1   1   0   0   3   1   0\n","    0   0   0]\n"," [  0   0   0   1   1   0   0   1   0   1   1   0   0   1   0   4   6   0\n","    0   0   1]\n"," [  0   0   0   2   2   0   2   2   0   0   0   0   0   1   1   6   2   0\n","    0   0   1]\n"," [  0   0   0  33   0   0   0  15   0   0   0  20   0   0   0 128  31   0\n","    0   0   4]\n"," [  0   0   0   2   0   0   0  21   0   0   0  12   0   1   0  11 131   0\n","    0   0  29]\n"," [  0   0   0   0   0   1   0   1   0   1   0   1   0   0   0   0   4   1\n","    0   5   7]\n"," [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   3   0\n","    1   4   7]\n"," [  0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   2   0\n","    1   4   4]\n"," [  0   0   0   5   0   0   0  20   0   1   0  20   0   0   0   2  33   1\n","    1   0 122]]\n","audio1.mp3 downloaded\n","audio2.mp3 downloaded\n","    audio_file    emotion        level  confidence\n","0   audio1.mp3      Anger  Unspecified        0.82\n","1   audio1.mp3    Disgust  Unspecified        0.05\n","2   audio1.mp3  Happiness  Unspecified        0.04\n","3   audio1.mp3       Fear  Unspecified        0.04\n","4   audio1.mp3    Neutral  Unspecified        0.01\n","5   audio1.mp3    Sadness  Unspecified        0.00\n","6   audio1.mp3      Anger         High        0.00\n","7   audio1.mp3    Disgust         High        0.00\n","8   audio1.mp3      Anger       Medium        0.00\n","9   audio1.mp3       Fear         High        0.00\n","10  audio1.mp3      Anger          Low        0.00\n","11  audio1.mp3  Happiness         High        0.00\n","12  audio1.mp3  Happiness       Medium        0.00\n","13  audio1.mp3       Fear       Medium        0.00\n","14  audio1.mp3    Sadness          Low        0.00\n","15  audio1.mp3    Disgust       Medium        0.00\n","16  audio1.mp3  Happiness          Low        0.00\n","17  audio1.mp3       Fear          Low        0.00\n","18  audio1.mp3    Sadness         High        0.00\n","19  audio1.mp3    Disgust          Low        0.00\n","20  audio1.mp3    Sadness       Medium        0.00\n","21  audio2.mp3      Anger  Unspecified        0.96\n","22  audio2.mp3    Disgust  Unspecified        0.01\n","23  audio2.mp3       Fear  Unspecified        0.01\n","24  audio2.mp3  Happiness         High        0.00\n","25  audio2.mp3    Neutral  Unspecified        0.00\n","26  audio2.mp3  Happiness  Unspecified        0.00\n","27  audio2.mp3      Anger         High        0.00\n","28  audio2.mp3      Anger          Low        0.00\n","29  audio2.mp3    Sadness  Unspecified        0.00\n","30  audio2.mp3      Anger       Medium        0.00\n","31  audio2.mp3    Disgust       Medium        0.00\n","32  audio2.mp3    Disgust         High        0.00\n","33  audio2.mp3  Happiness       Medium        0.00\n","34  audio2.mp3       Fear       Medium        0.00\n","35  audio2.mp3  Happiness          Low        0.00\n","36  audio2.mp3    Sadness         High        0.00\n","37  audio2.mp3       Fear         High        0.00\n","38  audio2.mp3       Fear          Low        0.00\n","39  audio2.mp3    Sadness       Medium        0.00\n","40  audio2.mp3    Disgust          Low        0.00\n","41  audio2.mp3    Sadness          Low        0.00\n"]}]}]}